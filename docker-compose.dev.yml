# Dev (Mac з GPU): Ollama локально, решта в Docker
# Запуск:
#   1. ollama serve (в окремому терміналі)
#   2. docker compose -f docker-compose.dev.yml up -d --build

services:
  chroma:
    image: chromadb/chroma:0.5.23
    container_name: rag-chroma
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - ANONYMIZED_TELEMETRY=false
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-app
    ports:
      - "8000:8000"
    volumes:
      - ./docs:/app/docs:ro
      - ./router_config.json:/app/router_config.json:ro
      - hf_cache:/root/.cache/huggingface
    env_file: .env
    environment:
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000
      - LLM_BASE_URL=http://host.docker.internal:11434/v1  # Локальний Ollama
      - LLM_MODEL=${LLM_MODEL:-qwen2.5:7b}
      - HF_HOME=/root/.cache/huggingface
    depends_on:
      chroma:
        condition: service_healthy
    restart: unless-stopped

volumes:
  chroma_data:
  hf_cache:
