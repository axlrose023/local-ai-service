# Local Corporate AI Assistant โ MVP Architecture

> **ะคะธะปะพัะพัะธั**: ะะฐะฟัััะธัั ัะฐะฑะพัะฐััะธะน ะฟัะพะดัะบั ะทะฐ 3 ะดะฝั, ะฐ ะฝะต ะธะดะตะฐะปัะฝัั ัะธััะตะผั ะทะฐ 3 ะผะตัััะฐ.

---

## ะะฑะทะพั

**3 ะบะพะฝัะตะนะฝะตัะฐ** ะฒะผะตััะพ 10:

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                     LOCAL NETWORK                            โ
โ                                                              โ
โ    โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ     โ
โ    โ                  APP (Monolith)                   โ     โ
โ    โ         Chainlit + FastAPI + Embeddings           โ     โ
โ    โ                    :8000                          โ     โ
โ    โโโโโโโโโโโโโโโโโโโโฌโโโโโโโโโโโโโโโโฌโโโโโโโโโโโโโโโโ     โ
โ                       โ               โ                      โ
โ              โโโโโโโโโโผโโโโโโโ โโโโโโโโผโโโโโโโโโ            โ
โ              โ   CHROMADB    โ โ   INFERENCE   โ            โ
โ              โ    :8001      โ โ  vLLM/Ollama  โ            โ
โ              โ               โ โ    :8002      โ            โ
โ              โโโโโโโโโโโโโโโโโ โโโโโโโโโโโโโโโโโ            โ
โ                                                              โ
โ    โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ      โ
โ    โ                ./docs (Volume)                   โ      โ
โ    โ         PDF/DOCX ัะฐะนะปั ะดะปั ะธะฝะดะตะบัะฐัะธะธ           โ      โ
โ    โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ      โ
โ                                                              โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

---

## ะงัะพ ะฒััะตะทะฐะฝะพ (ะธ ะฟะพัะตะผั)

| ะะพะผะฟะพะฝะตะฝั | ะกัะฐััั | ะัะธัะธะฝะฐ |
|-----------|--------|---------|
| Nginx Gateway | โ | Chainlit ะธะผะตะตั ะฒัััะพะตะฝะฝัะน auth |
| OIDC/SSO | โ | ะัะพััะพะน ะฟะฐัะพะปั ะฝะฐ ััะฐััะต |
| WAF/Rate Limiting | โ | ะะพะบะฐะปัะฝะฐั ัะตัั, ะดะพะฒะตัะตะฝะฝัะต ัะทะตัั |
| Hybrid Search (BM25) | โ | Vector search ะดะพััะฐัะพัะตะฝ ะดะปั <50k ะดะพะบัะผะตะฝัะพะฒ |
| Reranker | โ | top_k=5 ัะฐะฑะพัะฐะตั ะฝะพัะผะฐะปัะฝะพ |
| ACL ัะธะปัััะฐัะธั | โ | ะัะต ะดะพะบัะผะตะฝัั ะดะพัััะฟะฝั ะฒัะตะผ |
| TEI (Embedder service) | โ | sentence-transformers ะฒ ะฟัะพัะตััะต app |
| Redis | โ | ะกะตััะธะธ ะฒ ะฟะฐะผััะธ |
| Prometheus/Grafana/Loki | โ | docker logs ะดะพััะฐัะพัะฝะพ |
| Kubernetes | โ | Docker Compose |

---

## ะกัััะบัััะฐ ะฟัะพะตะบัะฐ

```
/local-ai-assistant
โโโ docker-compose.yml
โโโ Dockerfile
โโโ .env
โโโ .env.example
โ
โโโ app.py                    # ะขะพัะบะฐ ะฒัะพะดะฐ (Chainlit + FastAPI)
โโโ config.py                 # ะะฐัััะพะนะบะธ (Pydantic)
โโโ ingest.py                 # ะะฝะดะตะบัะฐัะธั ะดะพะบัะผะตะฝัะพะฒ
โโโ rag.py                    # ะะพะธัะบ ะฒ ะฑะฐะทะต ะทะฝะฐะฝะธะน
โโโ llm.py                    # ะะฑัะตะฝะธะต ั vLLM
โ
โโโ requirements.txt
โ
โโโ /docs                     # ะกัะดะฐ ะบะธะดะฐะตะผ PDF/DOCX
โ   โโโ example.pdf
โ
โโโ /chroma_data              # ะะฐะฝะฝัะต ChromaDB (git-ignored)
โ
โโโ /.chainlit
    โโโ config.toml           # ะะฐัััะพะนะบะธ UI
```

**ะัะตะณะพ 5 Python-ัะฐะนะปะพะฒ** ะฒะผะตััะพ 30+.

---

## Docker Compose

```yaml
# docker-compose.yml
version: "3.9"

services:
  # ========== APP (Monolith) ==========
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - LLM_BASE_URL=http://inference:8000/v1
      - LLM_MODEL=Qwen/Qwen2.5-32B-Instruct-AWQ
      - EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - DOCS_PATH=/app/docs
    volumes:
      - ./docs:/app/docs:ro
      - ./chroma_data:/app/chroma_data
    depends_on:
      chromadb:
        condition: service_healthy
      inference:
        condition: service_healthy
    restart: unless-stopped

  # ========== VECTOR DB ==========
  chromadb:
    image: chromadb/chroma:0.4.24
    volumes:
      - ./chroma_data:/chroma/chroma
    environment:
      - ANONYMIZED_TELEMETRY=false
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ========== LLM ==========
  inference:
    image: vllm/vllm-openai:latest
    command:
      - "--model"
      - "Qwen/Qwen2.5-32B-Instruct-AWQ"
      - "--quantization"
      - "awq"
      - "--max-model-len"
      - "8192"
      - "--gpu-memory-utilization"
      - "0.90"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
```

---

## Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# ะะฐะฒะธัะธะผะพััะธ ะดะปั PDF
RUN apt-get update && apt-get install -y \
    libmagic1 \
    poppler-utils \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# ะะพัั Chainlit
EXPOSE 8000

CMD ["chainlit", "run", "app.py", "--host", "0.0.0.0", "--port", "8000"]
```

---

## requirements.txt

```
# Core
chainlit>=1.0.0
fastapi>=0.109.0
uvicorn>=0.27.0
pydantic>=2.0.0
pydantic-settings>=2.0.0

# LLM
openai>=1.10.0

# RAG
chromadb>=0.4.24
sentence-transformers>=2.3.0

# Document parsing
pypdf>=4.0.0
python-docx>=1.1.0
python-magic>=0.4.27

# Utils
python-dotenv>=1.0.0
```

---

## ะะพะด

### config.py

```python
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    # ChromaDB
    chroma_host: str = "localhost"
    chroma_port: int = 8000
    chroma_collection: str = "corporate_docs"

    # LLM
    llm_base_url: str = "http://localhost:8000/v1"
    llm_model: str = "Qwen/Qwen2.5-32B-Instruct-AWQ"
    llm_max_tokens: int = 2048
    llm_temperature: float = 0.7

    # Embeddings
    embedding_model: str = "nomic-ai/nomic-embed-text-v1.5"

    # Documents
    docs_path: str = "./docs"

    # RAG
    rag_top_k: int = 5
    chunk_size: int = 500
    chunk_overlap: int = 50

    class Config:
        env_file = ".env"


settings = Settings()
```

### ingest.py

```python
"""
ะะฝะดะตะบัะฐัะธั ะดะพะบัะผะตะฝัะพะฒ ะฒ ChromaDB.
ะะฐะฟััะบ: python ingest.py
"""
import hashlib
from pathlib import Path

import chromadb
from pypdf import PdfReader
from docx import Document
from sentence_transformers import SentenceTransformer

from config import settings


def get_file_hash(file_path: Path) -> str:
    """MD5 ััั ัะฐะนะปะฐ ะดะปั ะพััะปะตะถะธะฒะฐะฝะธั ะธะทะผะตะฝะตะฝะธะน."""
    return hashlib.md5(file_path.read_bytes()).hexdigest()


def extract_text(file_path: Path) -> str:
    """ะะทะฒะปะตะบะฐะตั ัะตะบัั ะธะท PDF ะธะปะธ DOCX."""
    suffix = file_path.suffix.lower()

    if suffix == ".pdf":
        reader = PdfReader(file_path)
        return "\n".join(page.extract_text() or "" for page in reader.pages)

    elif suffix == ".docx":
        doc = Document(file_path)
        return "\n".join(para.text for para in doc.paragraphs)

    elif suffix in [".txt", ".md"]:
        return file_path.read_text(encoding="utf-8")

    return ""


def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list[str]:
    """ะะฐะทะฑะธะฒะฐะตั ัะตะบัั ะฝะฐ ัะฐะฝะบะธ ั ะฟะตัะตะบัััะธะตะผ."""
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # ะัะตะผ ะบะพะฝะตั ะฟัะตะดะปะพะถะตะฝะธั
        if end < len(text):
            for sep in [". ", ".\n", "\n\n", "\n"]:
                pos = text.rfind(sep, start, end)
                if pos != -1:
                    end = pos + len(sep)
                    break

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        start = end - overlap

    return chunks


def ingest_folder(folder_path: str = None):
    """ะะฝะดะตะบัะธััะตั ะฒัะต ะดะพะบัะผะตะฝัั ะธะท ะฟะฐะฟะบะธ."""
    folder = Path(folder_path or settings.docs_path)

    if not folder.exists():
        print(f"ะะฐะฟะบะฐ {folder} ะฝะต ัััะตััะฒัะตั")
        return

    # ะะพะดะบะปััะฐะตะผัั ะบ ChromaDB
    client = chromadb.HttpClient(
        host=settings.chroma_host,
        port=settings.chroma_port
    )

    # ะะพะปััะฐะตะผ ะธะปะธ ัะพะทะดะฐัะผ ะบะพะปะปะตะบัะธั
    collection = client.get_or_create_collection(
        name=settings.chroma_collection,
        metadata={"hnsw:space": "cosine"}
    )

    # ะะฐะณััะถะฐะตะผ ะผะพะดะตะปั ัะผะฑะตะดะดะธะฝะณะพะฒ
    print(f"ะะฐะณััะทะบะฐ ะผะพะดะตะปะธ {settings.embedding_model}...")
    embedder = SentenceTransformer(settings.embedding_model, trust_remote_code=True)

    # ะะพะปััะฐะตะผ ัััะตััะฒัััะธะต ัััะธ (ะดะปั ะธะฝะบัะตะผะตะฝัะฐะปัะฝะพะณะพ ะพะฑะฝะพะฒะปะตะฝะธั)
    existing = collection.get(include=["metadatas"])
    existing_hashes = {
        m.get("file_hash")
        for m in (existing.get("metadatas") or [])
        if m
    }

    # ะะฑัะฐะฑะฐััะฒะฐะตะผ ัะฐะนะปั
    extensions = ["*.pdf", "*.docx", "*.txt", "*.md"]
    files = []
    for ext in extensions:
        files.extend(folder.rglob(ext))

    print(f"ะะฐะนะดะตะฝะพ {len(files)} ัะฐะนะปะพะฒ")

    for file_path in files:
        file_hash = get_file_hash(file_path)

        # ะัะพะฟััะบะฐะตะผ ะตัะปะธ ะฝะต ะธะทะผะตะฝะธะปัั
        if file_hash in existing_hashes:
            print(f"โญ {file_path.name} โ ะฑะตะท ะธะทะผะตะฝะตะฝะธะน")
            continue

        print(f"๐ ะะฑัะฐะฑะพัะบะฐ {file_path.name}...")

        # ะะทะฒะปะตะบะฐะตะผ ัะตะบัั
        text = extract_text(file_path)
        if not text.strip():
            print(f"  โ ะัััะพะน ัะฐะนะป, ะฟัะพะฟััะบ")
            continue

        # ะะฐะทะฑะธะฒะฐะตะผ ะฝะฐ ัะฐะฝะบะธ
        chunks = chunk_text(text, settings.chunk_size, settings.chunk_overlap)
        print(f"  โ {len(chunks)} ัะฐะฝะบะพะฒ")

        # ะกะพะทะดะฐัะผ ัะผะฑะตะดะดะธะฝะณะธ
        embeddings = embedder.encode(chunks, show_progress_bar=False).tolist()

        # ะกะพััะฐะฝัะตะผ ะฒ ChromaDB
        ids = [f"{file_hash}_{i}" for i in range(len(chunks))]
        metadatas = [
            {
                "source": file_path.name,
                "file_path": str(file_path),
                "file_hash": file_hash,
                "chunk_index": i
            }
            for i in range(len(chunks))
        ]

        collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=chunks,
            metadatas=metadatas
        )

        print(f"  โ ะะพะฑะฐะฒะปะตะฝะพ ะฒ ะฑะฐะทั")

    print(f"\nโ ะะฝะดะตะบัะฐัะธั ะทะฐะฒะตััะตะฝะฐ. ะัะตะณะพ ะดะพะบัะผะตะฝัะพะฒ ะฒ ะฑะฐะทะต: {collection.count()}")


if __name__ == "__main__":
    ingest_folder()
```

### rag.py

```python
"""
RAG: ะฟะพะธัะบ ัะตะปะตะฒะฐะฝัะฝัั ะดะพะบัะผะตะฝัะพะฒ.
"""
import chromadb
from sentence_transformers import SentenceTransformer

from config import settings

# ะะปะพะฑะฐะปัะฝัะต ะพะฑัะตะบัั (ะธะฝะธัะธะฐะปะธะทะธัััััั ะพะดะธะฝ ัะฐะท)
_embedder: SentenceTransformer = None
_collection = None


def get_embedder() -> SentenceTransformer:
    global _embedder
    if _embedder is None:
        _embedder = SentenceTransformer(
            settings.embedding_model,
            trust_remote_code=True
        )
    return _embedder


def get_collection():
    global _collection
    if _collection is None:
        client = chromadb.HttpClient(
            host=settings.chroma_host,
            port=settings.chroma_port
        )
        _collection = client.get_or_create_collection(
            name=settings.chroma_collection,
            metadata={"hnsw:space": "cosine"}
        )
    return _collection


def search_documents(query: str, top_k: int = None) -> list[dict]:
    """
    ะัะตั ัะตะปะตะฒะฐะฝัะฝัะต ะดะพะบัะผะตะฝัั ะฟะพ ะทะฐะฟัะพัั.

    Returns:
        ะกะฟะธัะพะบ ัะปะพะฒะฐัะตะน ั ะบะปััะฐะผะธ: content, source, score
    """
    top_k = top_k or settings.rag_top_k

    # ะะพะปััะฐะตะผ ัะผะฑะตะดะดะธะฝะณ ะทะฐะฟัะพัะฐ
    embedder = get_embedder()
    query_embedding = embedder.encode(query).tolist()

    # ะัะตะผ ะฒ ChromaDB
    collection = get_collection()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )

    # ะคะพัะผะฐัะธััะตะผ ัะตะทัะปััะฐัั
    documents = []
    for i in range(len(results["ids"][0])):
        documents.append({
            "content": results["documents"][0][i],
            "source": results["metadatas"][0][i].get("source", "Unknown"),
            "score": 1 - results["distances"][0][i]  # cosine similarity
        })

    return documents


def format_context(documents: list[dict]) -> str:
    """ะคะพัะผะฐัะธััะตั ะดะพะบัะผะตะฝัั ะฒ ะบะพะฝัะตะบัั ะดะปั LLM."""
    if not documents:
        return "ะะตะปะตะฒะฐะฝัะฝัะต ะดะพะบัะผะตะฝัั ะฝะต ะฝะฐะนะดะตะฝั."

    parts = []
    for i, doc in enumerate(documents, 1):
        parts.append(f"[ะะพะบัะผะตะฝั {i}: {doc['source']}]\n{doc['content']}")

    return "\n\n".join(parts)


def get_sources(documents: list[dict]) -> list[str]:
    """ะะพะทะฒัะฐัะฐะตั ัะฟะธัะพะบ ัะฝะธะบะฐะปัะฝัั ะธััะพัะฝะธะบะพะฒ."""
    seen = set()
    sources = []
    for doc in documents:
        if doc["source"] not in seen:
            seen.add(doc["source"])
            sources.append(doc["source"])
    return sources
```

### llm.py

```python
"""
ะะปะธะตะฝั ะดะปั ะพะฑัะตะฝะธั ั LLM (vLLM).
"""
from typing import AsyncIterator

from openai import AsyncOpenAI

from config import settings

# ะะปะธะตะฝั OpenAI (vLLM ัะพะฒะผะตััะธะผ ั API)
client = AsyncOpenAI(
    base_url=settings.llm_base_url,
    api_key="not-needed"  # vLLM ะฝะต ััะตะฑัะตั ะบะปัั
)

SYSTEM_PROMPT = """ะขั โ ะบะพัะฟะพัะฐัะธะฒะฝัะน AI-ะฟะพะผะพัะฝะธะบ. ะขะฒะพั ะทะฐะดะฐัะฐ โ ะฟะพะผะพะณะฐัั ัะพัััะดะฝะธะบะฐะผ ะฝะฐัะพะดะธัั ะธะฝัะพัะผะฐัะธั ะฒ ะฑะฐะทะต ะทะฝะฐะฝะธะน ะบะพะผะฟะฐะฝะธะธ.

ะัะฐะฒะธะปะฐ:
1. ะัะฒะตัะฐะน ัะพะปัะบะพ ะฝะฐ ะพัะฝะพะฒะต ะฟัะตะดะพััะฐะฒะปะตะฝะฝะพะณะพ ะบะพะฝัะตะบััะฐ
2. ะัะปะธ ะธะฝัะพัะผะฐัะธะธ ะฝะตั ะฒ ะบะพะฝัะตะบััะต โ ัะตััะฝะพ ัะบะฐะถะธ ะพะฑ ััะพะผ
3. ะฃะบะฐะทัะฒะฐะน ะธััะพัะฝะธะบะธ ะธะฝัะพัะผะฐัะธะธ
4. ะัะฒะตัะฐะน ะฝะฐ ััััะบะพะผ ัะทัะบะต
5. ะัะดั ะบัะฐัะบะธะผ ะธ ะฟะพ ะดะตะปั"""


async def chat_stream(
    user_message: str,
    context: str = "",
    history: list[dict] = None
) -> AsyncIterator[str]:
    """
    ะกััะธะผะธะฝะณ ะพัะฒะตัะฐ ะพั LLM.

    Args:
        user_message: ะะพะฟัะพั ะฟะพะปัะทะพะฒะฐัะตะปั
        context: ะะพะฝัะตะบัั ะธะท RAG (ะดะพะบัะผะตะฝัั)
        history: ะััะพัะธั ะดะธะฐะปะพะณะฐ

    Yields:
        ะขะพะบะตะฝั ะพัะฒะตัะฐ
    """
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]

    # ะะพะฑะฐะฒะปัะตะผ ะธััะพัะธั
    if history:
        messages.extend(history)

    # ะคะพัะผะธััะตะผ ะฟัะพะผะฟั ั ะบะพะฝัะตะบััะพะผ
    if context:
        prompt = f"""ะะพะฝัะตะบัั ะธะท ะฑะฐะทั ะทะฝะฐะฝะธะน:
{context}

ะะพะฟัะพั ะฟะพะปัะทะพะฒะฐัะตะปั: {user_message}

ะะฐะน ะพัะฒะตั ะฝะฐ ะพัะฝะพะฒะต ะบะพะฝัะตะบััะฐ ะฒััะต."""
    else:
        prompt = user_message

    messages.append({"role": "user", "content": prompt})

    # ะะฐะฟัะพั ะบ LLM
    response = await client.chat.completions.create(
        model=settings.llm_model,
        messages=messages,
        max_tokens=settings.llm_max_tokens,
        temperature=settings.llm_temperature,
        stream=True
    )

    async for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content


async def should_search(user_message: str) -> bool:
    """
    ะัะพััะฐั ัะฒัะธััะธะบะฐ: ะฝัะถะตะฝ ะปะธ ะฟะพะธัะบ ะฒ ะฑะฐะทะต ะทะฝะฐะฝะธะน.

    ะะปั MVP ะธัะฟะพะปัะทัะตะผ ะฟัะพัััะต ะฟัะฐะฒะธะปะฐ ะฒะผะตััะพ ะพัะดะตะปัะฝะพะณะพ LLM ะฒัะทะพะฒะฐ.
    """
    # ะัะธะฒะตัััะฒะธั ะธ small talk โ ะฝะต ะธัะตะผ
    greetings = ["ะฟัะธะฒะตั", "ะทะดัะฐะฒััะฒัะน", "ะดะพะฑััะน ะดะตะฝั", "hi", "hello", "ะบะฐะบ ะดะตะปะฐ"]
    message_lower = user_message.lower().strip()

    if any(g in message_lower for g in greetings):
        return False

    # ะะพัะพัะบะธะต ัะพะพะฑัะตะฝะธั ะฑะตะท ะฒะพะฟัะพัะธัะตะปัะฝัั ัะปะพะฒ โ ะฝะต ะธัะตะผ
    if len(message_lower) < 10 and "?" not in message_lower:
        return False

    # ะัั ะพััะฐะปัะฝะพะต โ ะธัะตะผ
    return True
```

### app.py

```python
"""
ะะปะฐะฒะฝัะน ัะฐะนะป ะฟัะธะปะพะถะตะฝะธั.
Chainlit UI + RAG + LLM.
"""
import chainlit as cl

from config import settings
from rag import search_documents, format_context, get_sources
from llm import chat_stream, should_search


@cl.on_chat_start
async def start():
    """ะะฝะธัะธะฐะปะธะทะฐัะธั ัะตััะธะธ."""
    cl.user_session.set("history", [])

    await cl.Message(
        content="ะะดัะฐะฒััะฒัะนัะต! ะฏ ะบะพัะฟะพัะฐัะธะฒะฝัะน AI-ะฟะพะผะพัะฝะธะบ. "
                "ะะฐะดะฐะนัะต ะฒะพะฟัะพั ะฟะพ ะดะพะบัะผะตะฝัะฐะผ ะบะพะผะฟะฐะฝะธะธ, ะธ ั ะฟะพััะฐัะฐััั ะฟะพะผะพัั."
    ).send()


@cl.on_message
async def main(message: cl.Message):
    """ะะฑัะฐะฑะพัะบะฐ ัะพะพะฑัะตะฝะธั ะฟะพะปัะทะพะฒะฐัะตะปั."""
    user_input = message.content
    history = cl.user_session.get("history", [])

    # ะัะพะฒะตััะตะผ, ะฝัะถะตะฝ ะปะธ ะฟะพะธัะบ
    need_search = await should_search(user_input)

    context = ""
    sources = []

    if need_search:
        # ะะพะบะฐะทัะฒะฐะตะผ ััะพ ะธัะตะผ
        async with cl.Step(name="ะะพะธัะบ ะฒ ะฑะฐะทะต ะทะฝะฐะฝะธะน") as step:
            step.input = user_input

            # ะัะตะผ ะดะพะบัะผะตะฝัั
            documents = search_documents(user_input)

            if documents:
                context = format_context(documents)
                sources = get_sources(documents)
                step.output = f"ะะฐะนะดะตะฝะพ {len(documents)} ัะตะปะตะฒะฐะฝัะฝัั ััะฐะณะผะตะฝัะพะฒ"
            else:
                step.output = "ะะพะบัะผะตะฝัั ะฝะต ะฝะฐะนะดะตะฝั"

    # ะกะพะทะดะฐัะผ ัะพะพะฑัะตะฝะธะต ะดะปั ัััะธะผะธะฝะณะฐ
    msg = cl.Message(content="")
    await msg.send()

    # ะะตะฝะตัะธััะตะผ ะพัะฒะตั
    full_response = ""
    async for token in chat_stream(user_input, context, history):
        full_response += token
        await msg.stream_token(token)

    # ะะพะฑะฐะฒะปัะตะผ ะธััะพัะฝะธะบะธ
    if sources:
        sources_text = "\n\n---\n**ะััะพัะฝะธะบะธ:** " + ", ".join(sources)
        full_response += sources_text
        await msg.stream_token(sources_text)

    await msg.update()

    # ะกะพััะฐะฝัะตะผ ะธััะพัะธั (ะฟะพัะปะตะดะฝะธะต 10 ัะพะพะฑัะตะฝะธะน)
    history.append({"role": "user", "content": user_input})
    history.append({"role": "assistant", "content": full_response})
    cl.user_session.set("history", history[-10:])


@cl.on_stop
async def stop():
    """ะะพะปัะทะพะฒะฐัะตะปั ะฝะฐะถะฐะป Stop."""
    pass
```

### .chainlit/config.toml

```toml
[project]
name = "Corporate AI Assistant"
enable_telemetry = false

[UI]
name = "ะะพัะฟะพัะฐัะธะฒะฝัะน AI-ะฟะพะผะพัะฝะธะบ"
description = "ะะฐะดะฐะนัะต ะฒะพะฟัะพั ะฟะพ ะดะพะบัะผะตะฝัะฐะผ ะบะพะผะฟะฐะฝะธะธ"
default_theme = "light"
show_readme_as_default = false

[UI.theme]
primary = "#1976D2"
background = "#FFFFFF"
paper = "#F5F5F5"

[features]
spontaneous_file_upload = false
audio = false

[session]
timeout = 3600
```

### .env.example

```bash
# ChromaDB
CHROMA_HOST=chromadb
CHROMA_PORT=8000

# LLM (vLLM)
LLM_BASE_URL=http://inference:8000/v1
LLM_MODEL=Qwen/Qwen2.5-32B-Instruct-AWQ
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.7

# Embeddings
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# Documents
DOCS_PATH=./docs

# RAG
RAG_TOP_K=5
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Optional: Chainlit auth
# CHAINLIT_AUTH_SECRET=your-secret-key
```

---

## ะะฐะฟััะบ

### 1. ะะพะดะณะพัะพะฒะบะฐ

```bash
# ะะปะพะฝะธััะตะผ/ัะพะทะดะฐัะผ ะฟัะพะตะบั
mkdir local-ai-assistant && cd local-ai-assistant

# ะะพะฟะธััะตะผ ัะฐะนะปั (ะธะปะธ ัะพะทะดะฐัะผ ะฟะพ ัะฐะฑะปะพะฝะฐะผ ะฒััะต)

# ะกะพะทะดะฐัะผ .env
cp .env.example .env

# ะกะพะทะดะฐัะผ ะฟะฐะฟะบั ะดะปั ะดะพะบัะผะตะฝัะพะฒ
mkdir -p docs
# ะะพะฟะธััะตะผ ััะดะฐ PDF/DOCX ัะฐะนะปั
```

### 2. ะะฐะฟััะบ ะธะฝััะฐััััะบัััั

```bash
# ะะพะดะฝะธะผะฐะตะผ ะฒัะต ะบะพะฝัะตะนะฝะตัั
docker-compose up -d

# ะัะพะฒะตััะตะผ ััะฐััั
docker-compose ps

# ะกะผะพััะธะผ ะปะพะณะธ (ะพัะพะฑะตะฝะฝะพ inference โ ะผะพะดะตะปั ะณััะทะธััั ะดะพะปะณะพ)
docker-compose logs -f inference
```

### 3. ะะฝะดะตะบัะฐัะธั ะดะพะบัะผะตะฝัะพะฒ

```bash
# ะะฐัะพะดะธะผ ะฒ ะบะพะฝัะตะนะฝะตั app
docker-compose exec app bash

# ะะฐะฟััะบะฐะตะผ ะธะฝะดะตะบัะฐัะธั
python ingest.py
```

### 4. ะัะฟะพะปัะทะพะฒะฐะฝะธะต

ะัะบััะฒะฐะตะผ ะฒ ะฑัะฐัะทะตัะต: **http://localhost:8000**

---

## ะะปััะตัะฝะฐัะธะฒะฐ: Ollama ะฒะผะตััะพ vLLM

ะัะปะธ ัะพัะตััั ะตัั ะฟัะพัะต (ะฑะตะท ะฒะพะทะฝะธ ั CUDA/vLLM):

```yaml
# ะะฐะผะตะฝัะตะผ inference ะฒ docker-compose.yml
inference:
  image: ollama/ollama:latest
  volumes:
    - ollama_data:/root/.ollama
  ports:
    - "11434:11434"
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]

volumes:
  ollama_data:
```

ะ ะฒ `.env`:

```bash
LLM_BASE_URL=http://inference:11434/v1
LLM_MODEL=qwen2.5:32b
```

ะะพัะปะต ะทะฐะฟััะบะฐ:

```bash
# ะกะบะฐัะธะฒะฐะตะผ ะผะพะดะตะปั
docker-compose exec inference ollama pull qwen2.5:32b
```

---

## Roadmap: MVP โ Production

ะะพัะปะต ะทะฐะฟััะบะฐ MVP, ะดะพะฑะฐะฒะปัะตะผ ะฟะพ ะพะดะฝะพะน ัะธัะต ะทะฐ ัะฐะท:

### ะะตะดะตะปั 2: ะฃะปัััะตะฝะธะต ะบะฐัะตััะฒะฐ
- [ ] ะฃะฒะตะปะธัะธัั chunk_size ะดะพ 1000 ั overlap 200
- [ ] ะะพะฑะฐะฒะธัั ัะธะปัััะฐัะธั ะฟะพ ัะธะฟั ะดะพะบัะผะตะฝัะฐ
- [ ] ะฃะปัััะธัั ะฟัะพะผะฟั ะฝะฐ ะพัะฝะพะฒะต ัะธะดะฑะตะบะฐ

### ะะตะดะตะปั 3-4: ะะตะทะพะฟะฐัะฝะพััั
- [ ] ะะพะฑะฐะฒะธัั Chainlit OAuth (Google/GitHub)
- [ ] ะะปะธ ะฟัะพััะพะน Nginx ั Basic Auth ะฟะตัะตะด app

### ะะตััั 2: ะะฐะดัะถะฝะพััั
- [ ] ะะพะฑะฐะฒะธัั Redis ะดะปั ัะตััะธะน
- [ ] Health checks ะฝะฐ /health endpoint
- [ ] ะัะพััะพะน backup ัะบัะธะฟั ะดะปั chroma_data

### ะะตััั 3+: Enterprise (ัะผ. ARCHITECTURE.md)
- [ ] ACL ะฝะฐ ะดะพะบัะผะตะฝัะฐั
- [ ] Hybrid search
- [ ] ะะพะฝะธัะพัะธะฝะณ
- [ ] ะ ั.ะด.

---

## Troubleshooting

### vLLM ะฝะต ััะฐัััะตั / OOM

```bash
# ะัะพะฒะตััะตะผ GPU ะฟะฐะผััั
nvidia-smi

# ะฃะผะตะฝััะฐะตะผ ะบะพะฝัะตะบัั ะฒ docker-compose.yml
--max-model-len 4096  # ะฒะผะตััะพ 8192

# ะะปะธ ะฑะตััะผ ะผะพะดะตะปั ะฟะพะผะตะฝััะต
--model Qwen/Qwen2.5-14B-Instruct-AWQ
```

### ChromaDB connection refused

```bash
# ะัะพะฒะตััะตะผ ััะพ ะบะพะฝัะตะนะฝะตั ะทะฐะฟััะตะฝ
docker-compose ps chromadb

# ะัะพะฒะตััะตะผ health
curl http://localhost:8001/api/v1/heartbeat
```

### ะะตะดะปะตะฝะฝะฐั ะธะฝะดะตะบัะฐัะธั

ะะตัะฒัะน ะทะฐะฟััะบ sentence-transformers ัะบะฐัะธะฒะฐะตั ะผะพะดะตะปั (~500MB).
ะะพัะปะตะดัััะธะต ะทะฐะฟััะบะธ ะฑััััะตะต (ะผะพะดะตะปั ะบััะธััะตััั).

### Chainlit ะฝะต ะฒะธะดะธั ะธะทะผะตะฝะตะฝะธั ะฒ ะบะพะดะต

```bash
# ะะตัะตะทะฐะฟััะบะฐะตะผ ะบะพะฝัะตะนะฝะตั
docker-compose restart app

# ะะปะธ ะฟะตัะตัะพะฑะธัะฐะตะผ
docker-compose up -d --build app
```

---

## ะคะฐะนะปั ะดะปั ะบะพะฟะธัะพะฒะฐะฝะธั

ะัะต ัะฐะนะปั ะฒััะต ะณะพัะพะฒั ะบ ะธัะฟะพะปัะทะพะฒะฐะฝะธั. ะัะพััะพ:

1. ะกะพะทะดะฐะน ะฟะฐะฟะบั ะฟัะพะตะบัะฐ
2. ะกะบะพะฟะธััะน ะบะฐะถะดัะน ัะฐะนะป
3. `docker-compose up -d`
4. `docker-compose exec app python ingest.py`
5. ะัะบัะพะน http://localhost:8000

**ะัะตะผั ะดะพ ะฟะตัะฒะพะณะพ ัะฐะฑะพัะฐััะตะณะพ ัะฐัะฐ: ~30 ะผะธะฝัั** (+ ะฒัะตะผั ะทะฐะณััะทะบะธ ะผะพะดะตะปะธ).
